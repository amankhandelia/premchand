import torch
import numpy
import random

import numpy as np

from scipy.special import softmax
from vocab import HindiVocab
from network import PremchandLanguageModel



class TextGenerator:
    def __init__(self, model, vocab, strategy='argmax', top_n=1):
        """
        @param distribution[Sequence[float]]: A probability distribution over the set of token from which we wish to sample
        @param strategy[str]: Method which will operate on the distribution to sample the token
        """
        self.model = model
        self.vocab = vocab
        self.strategy = strategy
        self.top_n = top_n
        self.model.eval()

    def sample_token(self, distribution):
        """
        Sample a token based on the distribution and the strategy opted

        @return token[int]: Token sampled
        """
        # decoding strategies picked up from the following
        # https://youtu.be/4uG1NMKNWCU?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&t=1014

        if self.strategy is 'argmax':
            token = np.argmax(distribution)
        elif self.strategy is 'pure_sample':
            token = np.random.choice(numpy.arange(0, len(distribution)), p=distribution)
        elif self.strategy is 'top_n':
            top_n_dist = sorted(zip(range(0, len(distribution)), distribution), reverse=True, key=lambda x: x[1])[:self.top_n]
            distribution = softmax([item[1] for item in top_n_dist])
            choices = [item[0] for item in top_n_dist]
            token = np.random.choice(choices, p=distribution)
        return int(token)

    def generate_text(self, generate_n, prime_text=None, temperature=0.8):
        """
        Generate text based on the prime_text passed in.

        @param generate_n[int]: Number of tokens to be generated
        @param prime_text[str]: String for priming the model
        @param temperature[float]: For treating softmax

        @returns (original[str], generated_text[str]): A tuple of text, first one being the original text and other generated by the model based on the original text
        """
        original = ""
        state = (torch.zeros(1, 1, self.model.hidden_dim, dtype=torch.float),
                 torch.zeros(1, 1, self.model.hidden_dim, dtype=torch.float))
        sampled_tokens = []

        with torch.no_grad():
            if prime_text is not None:
                original = prime_text
                tokens = self.vocab.encode(original)
                for token in tokens[:-1]:
                    state = self.model.get_next_state(token, state)
                token = tokens[-1]
            else:
                token = random.randint(0, self.vocab.vocab_size - 1)
                original = self.vocab.decode([token])

            for i in range(generate_n):
                probs, state = self.model.get_next(token, state)
                token = self.sample_token(probs.numpy())
                sampled_tokens.append(token)

        generated_text = self.vocab.decode(sampled_tokens)
        return original, generated_text

    @classmethod
    def get_generator(cls, model_path, vocab_size, emb_dim, hidden_dim, strategy='argmax', top_n=1):
        model = PremchandLanguageModel.load_model(model_path, vocab_size, emb_dim, hidden_dim)
        vocab = HindiVocab.from_bpemb(vocab_size=vocab_size)
        return cls(model, vocab, strategy='argmax', top_n=1)